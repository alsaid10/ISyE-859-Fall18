## Areen Alsaid & Ammad Amin 

##### 1.  ##The problem we want to address involves detecting mind wandering through steering behavior and signals. 
We hope our model can help drivers recognize when their minds wander so they can become safer drivers. 

##### 2. The data was collected from 9 drivers as they were driving on a monotonous highway which can be a boring task. 
They were periodically probed and asked whether their mind was wandering or not (at the time of the probe). 
The data includes driving behavior data (speed, steering wheel angle, braking input, accelerator input, lane deviation) 
along with their response to the mind wandering probe which is our dependent variable.We invistigaetd the driving behavior
in the 10 seconds preceding a probe as predictor of mind wandering. We used two tyoes of features. 
First is simple common features (mean and standard deviation) of steering, accelarator input, speed and lane deviation. 
Second, we used more complex features such as entropy, spike, and curvature.

##### 3. The validity of our algorithm is very limited. According to the UTOS framework, here's a detailed explanation 
of why the validity is limited.
  U: Units -->  Our data was collected from 9 participant at the University of George Mason. 9 peopl is a very small 
  number that does not represent a population.
  T: Tratment --> All participants had the exact same driving scenario
  O: Observations --> The analyzed data only considered driving behavior (should in future invistigate EEG data as well.)
  S: Setting -->  The experimnet was run in a driving simulator, which may not apply to real driving conditions.

Also, construct validity is limited. Driver were periodically asked to report the state, however, we are not sure that they were actaully reporting mind wandering rather than fatigue.








####################### Basic Features #####################
#################################################
############### Prepare workspace ############### 
#################################################



library(tidyverse)
library(ggplot2)
library(caret)
library(caTools)
library(pROC)
library(devtools)
devtools::install_github("robjhyndman/tsfeatures") #to install package from github
library(tsfeatures)
devtools::install_github("robjhyndman/tscompdata") #to install package from github
library(tscompdata)
library(forecast)
library(psych)
devtools::install_github("robjhyndman/anomalous") #to install package from github
library(anomalous)
library(e1071)
library(ranger)



mindWanderingData = read_csv("/Users/areenalsaid/Google Drive/Research/Mind Wandering/R codes/Mind_Wandering_edits/mindWandering2/mindWanderingRawData.csv", col_names = TRUE)[, 1:21]


###### We are filtering out the 10 second after the probe (to only keep the 10 seconds prior)
mindWanderingData = mindWanderingData %>% filter(`preToneOrPostTone(1=preTone 2=postTone)` == '1')



mindWanderingData = mindWanderingData %>% filter(postWindowAfterEnd==0, driveTimestamp>100) # Remove data occur at end of drive or very start
mindWanderingData = mindWanderingData %>% filter(subject!=7) # Remove first drive of subject 7
names(mindWanderingData)[4]="state"

## Remove probe windows with incomplete data
mindWanderingData = mindWanderingData %>% group_by(subject, day, drive, probeNumber, state, `preToneOrPostTone(1=preTone 2=postTone)`) %>%
  mutate(n = n()) %>% filter(n==300)


###### Select the variables we want
mindWanderingData = mindWanderingData %>% select(c(1,2,3,4,11,13,16,17,19,20))


###### Rename the dependent variable


###### Group and summarize the data
mindWanderingData = mindWanderingData  %>% group_by(subject, day, drive, probeNumber, state) %>% summarise(steering.m = mean(steerInDegrees), speed.m = mean(speedInMPH), acc.m = mean(acceleratorInput), ld.m = mean(laneDeviation), steering.sd = sd(steerInDegrees), speed.sd = sd(speedInMPH), acc.sd = sd(acceleratorInput),ld.sd = sd(laneDeviation) )


###### Set seed for reproducability

set.seed(400)


###### Shuffle rows

mw.model = mindWanderingData[,-c(1,2,3,4)]
rows = sample(nrow(mw.model))
mw.model = mw.model[rows,]
mw.model$state = as.factor(mw.model$state)
levels(mw.model$state) = c("MW", "EN")


#### Split data into 80/20
split = round(nrow(mw.model)*0.8)
train = mw.model [1:split,]
test = mw.model[(split+1):nrow(mw.model),]

control <- trainControl(method="repeatedcv", number=5, repeats=1)
# train the LVQ model
set.seed(7)
modelranger1 <- train(state~., data=mw.model, method="ranger", trControl=control, importance = 'impurity')
# train the GBM model
set.seed(7)
modelGlm1 <- train(state~., data=mw.model, method="glm", family = "binomial",
                   trControl=control)
# train the SVM model
set.seed(7)
modelSvm1 <- train(state~., data=mw.model, method="svmRadial", trControl=control)
# collect resamples
results <- resamples(list(RandomForests=modelranger1, LogisticRegression = modelGlm1, SVM=modelSvm1))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results2)
# dot plots of results
dotplot(results2)




model.glm = glm(state ~., family = "binomial", train)


### Plot variance Importance

### Plot variance importance

var_importance = varImp(modelranger1)

v<-as.vector(varImp(modelranger1))
v = v$importance
#v = v[1:10,]
w<-(as.vector((colnames(mw.model[,-1]))))
#w= w[1:10]
DF<-cbind(w,v)
DF = as.data.frame(DF)
DF$Overall = as.numeric(DF$Overall)
DF = DF %>% filter(Overall>14)



#DF = DF[(1:10),]
simple = ggplot(DF, aes(x=reorder(w, Overall), y=Overall, fill = as.numeric(Overall)))+ 
  geom_bar(stat="identity", position="dodge")+ coord_flip()+
  ylab("Variable Importance")+
  xlab("")+
  ggtitle("Information Value Summary")+
  guides(fill=F)+
  theme_bw()
scale_fill_gradient(low="red", high="blue")
ggsave( "simple.png", device = "png", height = 4, width = 6)



################### Complex Features ####################



#################################################
############### Prepare workspace ############### 
#################################################

# Set working directory
setwd("~/Google Drive/Research/Mind Wandering/Journal_MW")

# Clean workspace and set seed
rm(list=ls()) 
set.seed(888)

# Load libraries
library(tidyverse)
library(ggplot2)
library(devtools)
devtools::install_github("robjhyndman/tsfeatures") #to install package from github
library(tsfeatures)
devtools::install_github("robjhyndman/tscompdata") #to install package from github
library(tscompdata)
library(forecast)
library(caret)
library(caTools)
library(pROC)
library(psych)
devtools::install_github("robjhyndman/anomalous") #to install package from github
library(anomalous)
library(mlbench)

# import the dataset
mw.df = read.csv("mindWanderingRawData.csv")

#################################################
############# Begining of the code ############## 
#################################################

# Remove data occur at end of drive or very start
names(mw.df)[4] = "state"
names(mw.df)[8] = "mwAwareness"
names(mw.df)[11] = "preToneOrPostTone"

mw.df$state[mw.df$state=="1"] = "mw"
mw.df$state[mw.df$state=="2"] = "ot"
mw.df$state = as.factor(mw.df$state)
mw.df$subject = as.factor(mw.df$subject)


mw.df = mw.df %>% filter(postWindowAfterEnd==0, driveTimestamp>100) # Remove data occur at end of drive or very start
mw.df = mw.df %>% filter(subject!=7) # Remove first drive of subject 7

## Remove probe windows with incomplete data
mw.df = mw.df %>% group_by(subject, day, drive, probeNumber, state, preToneOrPostTone) %>%
  mutate(n = n()) %>% filter(n==300)

# Select the variables we will be working with
mw.df = mw.df %>% select(c(1,2,3,4,11,13,16,17,19,20))

# Filter only 10s after the probe
mw.df = mw.df  %>% filter(preToneOrPostTone==1)


mw.df[11] = rep(1:1236,rep(300,1236))
test = mw.df %>% ungroup() %>% select(c(4,7,8,9,10,11))


## start a for loop to calculate features for each time series and store them in a list
## initialize the dataframe and name the coloumns 
features <- data.frame(matrix(ncol = 18, nrow = 12))
names(features)[1:15] = c("mean", "var", "x_acf1", "trend", "linearity", "curvature", 
                          "entropy", "lumpiness", "spike", "max_level_shift", "max_var_shift", 
                          "flat_spots", "crossing_points", "max_kl_shift", "time_kl_shift")

names(features)[16] = "state"
names(features)[17] = "subject"
names(features)[18] = "variable"

# initialize j
j=1

for (i in seq(1,370800,300)) {
  temp = mw.df[i:(i+299), c(7,8,9,10)]
  #state = mw.df[i,4]
  features[j:(j+3),(1:15)] <- bind_cols(
    tsfeatures(temp,
               c("acf_features","entropy","lumpiness",
                 "flat_spots","crossing_points")),
    tsfeatures(temp,"stl_features", s.window='periodic', 
               robust=TRUE),
    tsfeatures(temp, "max_kl_shift", width=48),
    tsfeatures(temp,
               c("mean","var"), scale=FALSE, na.rm=TRUE),
    tsfeatures(temp,
               c("max_level_shift","max_var_shift"), trim=TRUE)) %>%
    select(mean, var, x_acf1, trend, linearity, curvature, 
           entropy, lumpiness, spike, max_level_shift, max_var_shift, 
           flat_spots, crossing_points, max_kl_shift, time_kl_shift)
  print(j)
  
  features[j:(j+3),(16)] = mw.df[i,4]
  features[j:(j+3),(17)] = mw.df[i,1]
  features[j,(18)] = "speed"
  features[(j+1),(18)] = "steering"
  features[(j+2),(18)] = "accelerator"
  features[(j+3),(18)] = "lane.deviation"
  
  j = j+4
  print(i)
}

features$instance = rep(1:1236, each = 4)

### transform to long
names(features)[18]="x"
df= features %>% 
  gather(variable, value, -(c(state,subject, x, instance)))

df$var.name = NA
df2 = df %>% group_by(instance) %>% 
  unite( var.name, variable , x) %>% 
  spread(var.name, value)
  
###### Set seed for reproducability
set.seed(400)

#### Fit a model with the features data
#### Split data into 80/20

mw.complex = df2[,c(1, 4:63)]
mw.complex$state = as.factor(mw.complex$state)

control <- trainControl(method="repeatedcv", number=5, repeats=1)
# train the LVQ model
set.seed(7)
modelranger2 <- train(state~., data=mw.complex, method="ranger", trControl=control, importance = 'impurity')
# train the GBM model
set.seed(7)
modelGlm2 <- train(state~., data=mw.complex, method="glm", family = "binomial",
                   trControl=control)
# train the SVM model
set.seed(7)
modelSvm2 <- train(state~., data=mw.complex, method="svmRadial", trControl=control)
# collect resamples
results2 <- resamples(list(RandomForests=modelranger2, LogisticRegression = modelGlm2, SVM=modelSvm2))
# summarize the distributions
summary(results2)
# boxplots of results
bwplot(results2)
# dot plots of results
dotplot(results2)
modelranger2$modelInfo

### Plot variance importance

var_importance = varImp(modelranger2)

v<-as.vector(varImp(modelranger2))
v = v$importance
#v = v[1:10,]
w<-(as.vector((colnames(mw.complex[,-1]))))
#w= w[1:10]
DF<-cbind(w,v)
DF = as.data.frame(DF)
DF$Overall = as.numeric(DF$Overall)
DF = DF %>% filter(Overall>14)

#DF = DF[(1:10),]
ggplot(DF, aes(x=reorder(w, Overall), y=Overall, fill = as.numeric(Overall)))+ 
     geom_bar(stat="identity", position="dodge")+ coord_flip()+
     ylab("Variable Importance")+
     xlab("")+
     ggtitle("Information Value Summary")+
     guides(fill=F)+
  theme_bw()
  
ggsave( "complex.png", device = "png", height = 4, width = 6)


p = predict(model.glm2, test2, type = "response")
p_class2 = ifelse(p>0.7, 2, 1)

confusionMatrix(as.factor(p_class2),  as.factor(test2$state))

x=reorder(w,v)
x







